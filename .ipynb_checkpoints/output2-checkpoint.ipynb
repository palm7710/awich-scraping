{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290bb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVファイルを読み込み\n",
    "file_path = 'Awich_songs_df.csv'\n",
    "# 正しいエンコーディングを指定\n",
    "df = pd.read_csv(file_path, encoding='cp932')\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ファイルパスの設定\n",
    "main_dir = '/mnt/data'\n",
    "file_path = os.path.join('Awich_songs_df.csv')\n",
    "\n",
    "# CSVファイルの読み込み\n",
    "song_df = pd.read_csv(file_path, encoding='shift_jis')\n",
    "\n",
    "# song_idを一番左に持ってくる\n",
    "song_df['song_id'] = song_df.index + 1\n",
    "song_df = song_df[['song_id', 'song_name', 'lyrics']]\n",
    "\n",
    "# song_dfを転置してから辞書化する\n",
    "song_df_t = song_df[['song_name', 'lyrics']].T\n",
    "song_df_t.columns = song_df['song_id']\n",
    "song_dict = song_df_t.to_dict()\n",
    "\n",
    "# 辞書の構成を確認\n",
    "for key, val in song_dict[1].items():\n",
    "    print('◆{}:'.format(key), val)\n",
    "\n",
    "# song_id を 3 桁にフォーマット\n",
    "song_df['song_id'] = song_df['song_id'].apply(lambda x: f'{x:03}')\n",
    "\n",
    "# song_df を転置してから辞書化\n",
    "song_df_t = song_df[['song_name', 'lyrics']].T\n",
    "song_df_t.columns = song_df['song_id']\n",
    "song_dict = song_df_t.to_dict()\n",
    "\n",
    "# song_id リストの確認\n",
    "song_id_list = list(song_dict.keys())\n",
    "print(song_id_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d10c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from janome.tokenizer import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# nltkのデータをダウンロード\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 形態素解析のためのトークナイザー\n",
    "t = Tokenizer()\n",
    "\n",
    "# 残したい日本語の品詞のリスト\n",
    "pos_list = ['名詞', '形容詞', '副詞', '動詞', '連体詞', '接続詞']\n",
    "\n",
    "# 英語の品詞のマッピング\n",
    "pos_map = {\n",
    "    'NN': '名詞', 'JJ': '形容詞', 'RB': '副詞', 'VB': '動詞',\n",
    "    'DT': '連体詞', 'IN': '接続詞', 'PRP': '代名詞', 'CC': '接続詞'\n",
    "}\n",
    "\n",
    "# 英語の単語を抽出する正規表現\n",
    "english_word_re = re.compile(r'[a-zA-Z]+')\n",
    "\n",
    "# 分かち書きされた単語を正規化し、不要文字列を除外するための関数\n",
    "def wd_cleaning(wd):\n",
    "    # 文字列の正規化（全角を半角へ、アルファベットは小文字にする）\n",
    "    wd = unicodedata.normalize('NFKC', wd).lower()\n",
    "    # 不要文字列を''に置換するための正規表現（アルファベット、改行コード、記号など）\n",
    "    code_regex = re.compile(r'[0-9#&!,?\\'()~\\-...…;:“]')\n",
    "    cleaned_wd = code_regex.sub('', wd)\n",
    "    return cleaned_wd\n",
    "\n",
    "# 歌詞を形態素解析するための関数\n",
    "def lyric_cleaning(lyric):\n",
    "    # 英語の単語を抽出\n",
    "    english_words = english_word_re.findall(lyric)\n",
    "    english_words = [word.lower() for word in english_words if wd_cleaning(word) != '']\n",
    "\n",
    "    # 日本語の形態素解析を行い、指定した品詞に限り、[単語の原形, 品詞]のセットをリストで返す\n",
    "    lyric_sep = [[wd.base_form, wd.part_of_speech.split(\",\")[0]] for wd in t.tokenize(lyric) if wd.part_of_speech.split(\",\")[0] in pos_list]\n",
    "\n",
    "    # 英語の品詞タグ付けを行う\n",
    "    english_tags = pos_tag(english_words)\n",
    "    english_tags = [(word, pos_map.get(tag[:2], 'その他')) for word, tag in english_tags]\n",
    "\n",
    "    # 不要文字列を除外\n",
    "    lyric_sep = [wd for wd in lyric_sep if wd_cleaning(wd[0]) != '']\n",
    "    # 改行コードが除外しきれていないことがあるので、含まれている場合は除外する\n",
    "    lyric_sep = [wd for wd in lyric_sep if '\\u3000' not in wd[0]]\n",
    "    # 意味をなさない言葉が残ることがあるので、除外する\n",
    "    kana_list = [chr(i) for i in range(12353, 12436)]\n",
    "    stopwds_list = kana_list + ['ー', 'する', 'いる', 'てる', 'なる', 'れる', 'られる', 'せる']\n",
    "    lyric_sep = [wd for wd in lyric_sep if wd[0] not in stopwds_list]\n",
    "\n",
    "    # 英語の単語を追加\n",
    "    lyric_sep.extend(english_tags)\n",
    "\n",
    "    return lyric_sep\n",
    "\n",
    "# 例として、歌詞をクレンジングしてみる\n",
    "sample_lyric = \"Pandemic Plandemic地球よ　are you ready？消えてくmoney　増えてく闇誰が敵で、誰が family？Fathers がいない子供達Mothers を責めて指を刺し悪魔は boys & girls に hush恐怖に怯える面差しSo, where we going？\"\n",
    "cleaned_lyric = lyric_cleaning(sample_lyric)\n",
    "print(cleaned_lyric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各 song_id に対して歌詞を形態素解析し、辞書に追加する\n",
    "for song_id in song_id_list:\n",
    "    lyric = song_dict[song_id]['lyrics']\n",
    "\n",
    "    # テキストを分かち書きし、除外指定した品詞以外の形態素を残す\n",
    "    lyric_sep = lyric_cleaning(lyric)\n",
    "\n",
    "    # 単語と品詞のリスト（集計用）と、単語を空白でjoinした文字列（モデル構築用）を、辞書に追加する\n",
    "    lyric_sep_wds = [wds for wds, pos in lyric_sep]\n",
    "    lyric_sep_pos = [pos for wds, pos in lyric_sep]\n",
    "    song_dict[song_id]['lyric_wds'] = lyric_sep_wds\n",
    "    song_dict[song_id]['lyric_pos'] = lyric_sep_pos\n",
    "    song_dict[song_id]['lyric_wds_join'] = '　'.join(lyric_sep_wds)\n",
    "\n",
    "# 辞書の構成を再確認\n",
    "sample_song_id = song_id_list[0]\n",
    "for key, val in song_dict[sample_song_id].items():\n",
    "    print('◆{}:'.format(key), val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書きされた単語と品詞を抽出してリストにする\n",
    "lyric_wds_list = [song_dict[song_id]['lyric_wds'] for song_id in song_id_list]\n",
    "lyric_pos_list = [song_dict[song_id]['lyric_pos'] for song_id in song_id_list]\n",
    "songids_list = [[song_id] * len(lyric_wds) for song_id, lyric_wds in zip(song_id_list, lyric_wds_list)]\n",
    "\n",
    "# リストが二次元になっているので、一元化する\n",
    "lyric_wds_list_flatten = sum(lyric_wds_list, [])\n",
    "lyric_pos_list_flatten = sum(lyric_pos_list, [])\n",
    "songids_list_flatten = sum(songids_list, [])\n",
    "\n",
    "# データフレームにする\n",
    "lyric_df = pd.DataFrame({\n",
    "    'song_id': songids_list_flatten,\n",
    "    'wd': lyric_wds_list_flatten,\n",
    "    'pos': lyric_pos_list_flatten\n",
    "})\n",
    "\n",
    "# song_dict から曲名を取得する関数\n",
    "def get_song_name(song_id):\n",
    "    return song_dict[song_id]['song_name']\n",
    "\n",
    "# 曲名をデータフレームに追加\n",
    "lyric_df['song_name'] = lyric_df['song_id'].apply(get_song_name)\n",
    "\n",
    "# 曲名×品詞ごとに単語の出現回数をカウントする\n",
    "lyric_df_gp = lyric_df.groupby(['song_name', 'pos', 'wd']).size().to_frame('cnt').reset_index()\n",
    "lyric_df_gp = lyric_df_gp.sort_values(by=['song_name', 'pos', 'cnt'], ascending=[True, True, False]).reset_index(drop=True)\n",
    "\n",
    "# 曲名×品詞ごとに出現回数のランクをつける\n",
    "lyric_df_gp['cnt_rank'] = lyric_df_gp.groupby(['song_name', 'pos'])['cnt'].rank(ascending=False)\n",
    "\n",
    "# スプレッドシート形式でデータフレームを表示\n",
    "import IPython.display as display\n",
    "display.display(lyric_df_gp.query('pos == \"形容詞\"').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a75593",
   "metadata": {},
   "outputs": [],
   "source": [
    "曲名ごとに、出現回数TOP30の単語を棒グラフで表示する関数\n",
    "def wdcnt_graph_show(target_pos):\n",
    "\n",
    "  # 指定した品詞のデータを抽出\n",
    "  target_df =  lyric_df_gp.query('pos == @target_pos')\n",
    "\n",
    "  # 曲ごとにTOP30の品詞と単語のリストを抽出してリストに格納\n",
    "  ginfo_list = []\n",
    "  for artist_name in artist_name_list:\n",
    "    # TOP30を抽出した後、昇順で並び替え（横棒グラフで降順で表示するための処理）\n",
    "    artist_df = target_df.query('artist_name == @artist_name and cnt_rank <= 30').sort_values(by='cnt',ascending=True)\n",
    "    ginfo_list.append([artist_name, artist_df['wd'].tolist(), artist_df['cnt'].tolist()])\n",
    "\n",
    "  # 曲ごとに横棒グラフで表示\n",
    "  fig = plt.figure(figsize=(30,15))\n",
    "  plt.suptitle('【{}における出現回数】'.format(target_pos), fontsize=30)\n",
    "\n",
    "  for idx, ginfo in enumerate(ginfo_list):\n",
    "    artist_name, wd_list, cnt_list = ginfo[0], ginfo[1], ginfo[2]\n",
    "\n",
    "    # ランキングする単語数を取得（品詞によってはTOP30もないことがあるので）\n",
    "    rank_num = len(wd_list)\n",
    "\n",
    "    # 出現回数のラベル用\n",
    "    cnt_labels = np.linspace(0, max(cnt_list), 5)\n",
    "    # print(cnt_labels)\n",
    "    # >>>[  0.   30.5  61.   91.5 122. ]\n",
    "\n",
    "    # 棒グラフを描画\n",
    "    ax = fig.add_subplot(1, 3, idx+1)\n",
    "    ax.set_title(artist_name, fontsize=25)\n",
    "    ax.barh(wd_list, cnt_list)\n",
    "\n",
    "    # set_ytics,set_xticks：ラベルの位置を指定\n",
    "    # set_yticslabels,set_xtickslabels：ラベルの値とフォントサイズを指定\n",
    "    # ※set_ytics,set_xticksを指定せずに実行するとwarningが出るので注意\n",
    "    ax.set_yticks(range(rank_num))\n",
    "    ax.set_yticklabels(wd_list,fontsize=20)\n",
    "    ax.set_xticks(cnt_labels)\n",
    "    ax.set_xticklabels(cnt_labels,fontsize=20)\n",
    "    ax.grid(linestyle='dotted', linewidth=1,axis='x',color=\"r\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38427929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import font_manager\n",
    "\n",
    "# 日本語フォントを設定\n",
    "font_path = '/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc'  # ここにはシステムにインストールされている日本語フォントのパスを指定してください\n",
    "font_prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = font_prop.get_name()\n",
    "\n",
    "# 曲名のリストを取得\n",
    "song_name_list = lyric_df['song_name'].unique().tolist()\n",
    "\n",
    "# 出現回数TOP30の単語を棒グラフで表示する関数\n",
    "def wdcnt_graph_show(target_pos):\n",
    "    # 指定した品詞のデータを抽出\n",
    "    target_df = lyric_df_gp.query('pos == @target_pos')\n",
    "\n",
    "    # 曲ごとにTOP30の品詞と単語のリストを抽出してリストに格納\n",
    "    for song_name in song_name_list:\n",
    "        # TOP30を抽出した後、昇順で並び替え（横棒グラフで降順で表示するための処理）\n",
    "        song_df = target_df.query('song_name == @song_name and cnt_rank <= 30').sort_values(by='cnt', ascending=True)\n",
    "        if not song_df.empty:\n",
    "            wd_list = song_df['wd'].tolist()\n",
    "            cnt_list = song_df['cnt'].tolist()\n",
    "\n",
    "            # ランキングする単語数を取得（品詞によってはTOP30もないことがあるので）\n",
    "            rank_num = len(wd_list)\n",
    "\n",
    "            # 出現回数のラベル用\n",
    "            if cnt_list:\n",
    "                cnt_labels = np.linspace(0, max(cnt_list), 5)\n",
    "            else:\n",
    "                cnt_labels = []\n",
    "\n",
    "            # 新しいプロットを作成\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            ax.set_title(f'{song_name} - {target_pos}における出現回数', fontsize=20, fontproperties=font_prop)\n",
    "            ax.barh(wd_list, cnt_list)\n",
    "\n",
    "            # ラベルの位置と値を設定\n",
    "            ax.set_yticks(range(rank_num))\n",
    "            ax.set_yticklabels(wd_list, fontsize=12, fontproperties=font_prop)\n",
    "            ax.set_xticks(cnt_labels)\n",
    "            ax.set_xticklabels(cnt_labels, fontsize=12)\n",
    "            ax.grid(linestyle='dotted', linewidth=1, axis='x', color=\"r\")\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "\n",
    "# グラフを画像ファイルとして保存\n",
    "            img_path = f\"{song_name}.png\"\n",
    "            plt.savefig(img_path, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "            # 画像をエクセルに挿入\n",
    "            img = Image(img_path)\n",
    "            ws.add_image(img, f\"A{song_name_list.index(song_name) * 20 + 1}\")\n",
    "\n",
    "wdcnt_graph_show(\"名詞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdcnt_graph_show(\"副詞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 指定された曲の分かち書きされた歌詞を一つの文字列として返す関数\n",
    "def get_lyrics(song_id):\n",
    "    lyrics_join = song_dict[song_id]['lyric_wds_join']\n",
    "    return lyrics_join\n",
    "\n",
    "# WordCloudのパラメーターを設定\n",
    "my_font_path = '/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc'  # ここにはシステムにインストールされている日本語フォントのパスを指定してください\n",
    "stop_wds = ['こと', 'そう', 'ない', 'いい', 'くれる', 'この', '何', 'ある', 'よう', 'ちゃう', 'まま', 'もの', \n",
    "            'the', 'and', 'for', 'with', 'but', 'not', 'you', 'your', 'about', 'from', 'they', 'will', 'this', \n",
    "            'that', 'there', 'which', 'would', 'can', 'could', 'should', 'these', 'those', 'been', 'their', \n",
    "            'what', 'when', 'where', 'who', 'why', 'how', 'its', 'here', 'have', 'has', 'had', 'did', 'does']\n",
    "\n",
    "color_list = ['cividis', 'cool_r', 'winter']\n",
    "\n",
    "# 曲ごとにWordCloudを表示\n",
    "for idx, song_id in enumerate(song_id_list):\n",
    "    # 曲データを取得\n",
    "    lyrics_join = get_lyrics(song_id)\n",
    "\n",
    "    # テーマカラー\n",
    "    theme_color = color_list[idx % len(color_list)]\n",
    "\n",
    "    # インスタンスを生成\n",
    "    word_cloud = WordCloud(\n",
    "        font_path=my_font_path,  # フォントファイルのパス\n",
    "        background_color='white',\n",
    "        colormap=theme_color,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        stopwords=set(stop_wds),\n",
    "        max_words=200,\n",
    "        min_font_size=15,\n",
    "        max_font_size=70,\n",
    "        prefer_horizontal=1  # 横書きで配置\n",
    "    )\n",
    "\n",
    "    # 文字列を与えてWordCloud画像を生成\n",
    "    output_img = word_cloud.generate(lyrics_join)\n",
    "\n",
    "    # WordCloudを描画\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(output_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(song_dict[song_id]['song_name'], fontsize=30)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e71a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d58bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
